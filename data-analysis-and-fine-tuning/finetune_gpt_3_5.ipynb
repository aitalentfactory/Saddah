{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC4gsj8GwjTQ",
        "outputId": "cd94e461-b36a-45bf-d26b-b1e8b0cd10fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.19.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcMWKZ6pz9jr",
        "outputId": "2be946a7-ec01-4463-bd54-d0090d34bc36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.52)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Using cached pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.31)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (2.33.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.21 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import libraries\n",
        "import openai\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "LxjzcbaQwu4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Set OpenAI API key\n",
        "openai.api_key = \"\"  # Replace with your actual key securely\n",
        "\n",
        "# Optional: store in env\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ],
      "metadata": {
        "id": "w82oUNBhw0r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning LLM Training Model"
      ],
      "metadata": {
        "id": "QI96Gm6pBSML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare dataset for Training Model"
      ],
      "metadata": {
        "id": "T08EfSGEBZif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Convert dataset from prompt-completion to chat format\n",
        "\n",
        "input_path = \"/content/drive/MyDrive/dataset-competition/footballers_finetuning.jsonl\"            # original file\n",
        "formatted_path = \"/content/drive/MyDrive/dataset-competition/footballers_finetuning_formatted_dataset.jsonl\"  # Output file\n",
        "\n",
        "with open(input_path, \"r\") as infile, open(formatted_path, \"w\") as outfile:\n",
        "    for line in infile:\n",
        "        data = json.loads(line)\n",
        "        chat_data = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": data[\"prompt\"]},\n",
        "                {\"role\": \"assistant\", \"content\": data[\"completion\"]}\n",
        "            ]\n",
        "        }\n",
        "        json.dump(chat_data, outfile)\n",
        "        outfile.write(\"\\n\")\n",
        "\n",
        "print(\"Dataset formatted for chat model.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-9X21jtw4wK",
        "outputId": "1d5d2400-719f-4960-d908-522e5b2abd8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset formatted for chat model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Load JSONL data\n",
        "input_path = \"/content/drive/MyDrive/dataset-competition/footballers_finetuning_formatted_dataset.jsonl\"  # replace with actual file path\n",
        "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "# Shuffle and split (70% train, 30% test)\n",
        "random.seed(42)\n",
        "random.shuffle(data)\n",
        "split_idx = int(len(data) * 0.7)\n",
        "train_data = data[:split_idx]\n",
        "test_data = data[split_idx:]\n",
        "\n",
        "# Save training set\n",
        "with open(\"footballers_finetuning_train_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in train_data:\n",
        "        json.dump(item, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "# Save testing set\n",
        "with open(\"footballers_finetuning_test_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in test_data:\n",
        "        json.dump(item, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(f\"✅ Split complete: {len(train_data)} training / {len(test_data)} testing examples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlfldbX0DcNP",
        "outputId": "51f130d4-6008-459b-ee89-9351a5ecf17e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Split complete: 11091 training / 4754 testing examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tunning"
      ],
      "metadata": {
        "id": "wSekUQKRBzhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {openai.api_key}\"\n",
        "}\n",
        "\n",
        "# Open the file in binary mode\n",
        "with open(formatted_path, \"rb\") as f:\n",
        "    response = requests.post(\n",
        "        \"https://api.openai.com/v1/files\",\n",
        "        headers=headers,\n",
        "        files={\"file\": f},\n",
        "        data={\"purpose\": \"fine-tune\"}\n",
        "    )\n",
        "\n",
        "# Convert the response to JSON and extract the file id\n",
        "upload_response = response.json()\n",
        "file_id = upload_response.get(\"id\")\n",
        "print(\"Uploaded File ID:\", file_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkhpbuUHxFnO",
        "outputId": "4ca7f57d-0c89-4074-bf83-e3e4cfbb9ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📤 Uploaded File ID: file-XEWG7mq6HAVZgkdScfZSFK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate fine-tuning job using the uploaded file ID\n",
        "fine_tune_response = openai.FineTuningJob.create(\n",
        "    training_file=file_id,  # Use the uploaded file ID\n",
        "    model=\"gpt-3.5-turbo-1106\",     # Use this or latest supported\n",
        "    suffix=\"footballers-finetuning\" # Provide a suffix for your fine-tuned model\n",
        ")\n",
        "\n",
        "job_id = fine_tune_response[\"id\"]\n",
        "print(\"Fine-tuning job started with Job ID:\", job_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US167oGryD2X",
        "outputId": "d3c317d7-5354-44c8-88d7-39713db2ef6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Fine-tuning job started with Job ID: ftjob-EIzpvQcjPMZUIAqs3G4ycb8a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Monitor fine-tuning job status\n",
        "def check_job_status(job_id):\n",
        "    job_info = openai.FineTuningJob.retrieve(job_id)\n",
        "    status = job_info[\"status\"]\n",
        "    print(f\"🕒 Status: {status}\")\n",
        "    if status == \"succeeded\":\n",
        "        print(\"✅ Fine-tuning completed successfully!\")\n",
        "        print(\"Model name:\", job_info[\"fine_tuned_model\"])\n",
        "    elif status == \"failed\":\n",
        "        print(\"❌ Fine-tuning failed.\")\n",
        "    return job_info\n",
        "\n",
        "# Re-run this cell periodically to check status\n",
        "job_info = check_job_status(job_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDPk0_k9xJJ9",
        "outputId": "db4920e3-eae7-4cc1-d421-cfee12078c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🕒 Status: succeeded\n",
            "✅ Fine-tuning completed successfully!\n",
            "Model name: ft:gpt-3.5-turbo-1106:personal:footballers-finetuning:BIxFJYux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(job_info[\"status\"])\n",
        "print(job_info[\"fine_tuned_model\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIrTpd7C1uko",
        "outputId": "1fae6e1b-cab9-4385-f5f5-cb2fab7af2e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "succeeded\n",
            "ft:gpt-3.5-turbo-1106:personal:footballers-finetuning:BIxFJYux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm_model = ChatOpenAI(\n",
        "    model=job_info[\"fine_tuned_model\"],  # or paste the model name manually\n",
        "    openai_api_key=openai.api_key\n",
        ")\n",
        "\n",
        "response = llm_model.invoke(\"Create a 1-week training program for Faisal Khan\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zET6amyuzw6n",
        "outputId": "f4457c59-a05e-4ce0-b099-b747e98c881f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-a8bc4b30006e>:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm_model = ChatOpenAI(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Day 1: \n",
            "- Warm-up: 5 minutes of light jogging\n",
            "- Strength training: 3 sets of push-ups, 3 sets of squats\n",
            "- Cardio: 20 minutes of cycling\n",
            "\n",
            "Day 2: \n",
            "- Warm-up: 5 minutes of jumping jacks\n",
            "- Strength training: 3 sets of lunges, 3 sets of tricep dips\n",
            "- Cardio: 20 minutes of running\n",
            "\n",
            "Day 3: \n",
            "- Rest day\n",
            "\n",
            "Day 4: \n",
            "- Warm-up: 5 minutes of dynamic stretching\n",
            "- Strength training: 3 sets of pull-ups, 3 sets of deadlifts\n",
            "- Cardio: 20 minutes of swimming\n",
            "\n",
            "Day 5: \n",
            "- Warm-up: 5 minutes of high knees\n",
            "- Strength training: 3 sets of bicep curls, 3 sets of shoulder presses\n",
            "- Cardio: 20 minutes of rowing\n",
            "\n",
            "Day 6: \n",
            "- Rest day\n",
            "\n",
            "Day 7: \n",
            "- Warm-up: 5 minutes of yoga or stretching\n",
            "- Cardio: 30 minutes of hiking or brisk walking.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Accuracy"
      ],
      "metadata": {
        "id": "MB-Vda6qENkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n",
        "def extract_overall_rating(text):\n",
        "    match = re.search(r\"Overall\\s*\\(?(\\d{2,3})\\)?\", text)\n",
        "    return int(match.group(1)) if match else None\n",
        "\n",
        "def categorize_rating(rating):\n",
        "    if rating is None:\n",
        "        return \"Unknown\"\n",
        "    elif rating >= 90:\n",
        "        return \"High\"\n",
        "    elif rating >= 80:\n",
        "        return \"Mid\"\n",
        "    else:\n",
        "        return \"Low\"\n",
        "\n",
        "def evaluate_finetuned_model_confusion_matrix(test_file_path, model_name, api_key):\n",
        "    llm_model = ChatOpenAI(\n",
        "        model=model_name,\n",
        "        openai_api_key=api_key,\n",
        "        temperature=0  # deterministic output\n",
        "    )\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with open(test_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            sample = json.loads(line)\n",
        "            messages = sample[\"messages\"]\n",
        "            expected_text = messages[-1][\"content\"]\n",
        "            prompt_messages = messages[:-1]\n",
        "\n",
        "            # Convert messages to LangChain format\n",
        "            lc_messages = []\n",
        "            for msg in prompt_messages:\n",
        "                if msg[\"role\"] == \"system\":\n",
        "                    lc_messages.append(SystemMessage(content=msg[\"content\"]))\n",
        "                elif msg[\"role\"] == \"user\":\n",
        "                    lc_messages.append(HumanMessage(content=msg[\"content\"]))\n",
        "\n",
        "            # Extract true label\n",
        "            true_rating = extract_overall_rating(expected_text)\n",
        "            y_true.append(categorize_rating(true_rating))\n",
        "\n",
        "            # Invoke model\n",
        "            try:\n",
        "                response = llm_model.invoke(lc_messages)\n",
        "                predicted_text = response.content\n",
        "                predicted_rating = extract_overall_rating(predicted_text)\n",
        "                y_pred.append(categorize_rating(predicted_rating))\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error: {e}\")\n",
        "                y_pred.append(\"Unknown\")\n",
        "\n",
        "    print(confusion_matrix(y_true, y_pred, labels=[\"High\", \"Mid\", \"Low\"]))"
      ],
      "metadata": {
        "id": "oezY-QXVz2Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_finetuned_model_confusion_matrix(\n",
        "    test_file_path=\"/content/drive/MyDrive/dataset-competition/footballers_finetuning_test_data.jsonl\",\n",
        "    model_name=job_info[\"fine_tuned_model\"],\n",
        "    api_key=openai.api_key\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5g_SWoZG4gG",
        "outputId": "268a91e0-7ed8-4aa7-a058-ba68b926727e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Metrics::\n",
            "Accuracy : 0.9701\n",
            "Precision: 0.9885\n",
            "Recall   : 0.9536\n",
            "F1 Score : 0.9708\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FIne Tuning Ranking Model"
      ],
      "metadata": {
        "id": "sNJmJcvLwXfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset"
      ],
      "metadata": {
        "id": "hNgRzEp-xcDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load player data from CSV\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/dataset-competition/male_players.csv\")\n",
        "\n",
        "# Sort for ranking purposes\n",
        "df_sorted = df.sort_values(by=\"Overall\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "\n",
        "# Helper: Generate player evaluation example\n",
        "def generate_evaluation(row):\n",
        "    name = row['Name']\n",
        "    age = row['Age']\n",
        "    club = row['Club']\n",
        "    overall = row['Overall']\n",
        "    shooting = row['Shooting']\n",
        "    pace = row['Pace']\n",
        "    physicality = row['Physicality']\n",
        "\n",
        "    prompt = f\"Evaluate the player {name} (Age: {age}, Club: {club}) based on attributes like Shooting, Pace, and Physicality.\"\n",
        "    response = (\n",
        "        f\"{name} is a top player for {club}. \"\n",
        "        f\"At age {age}, he has an Overall rating of {overall}. \"\n",
        "        f\"He excels in Shooting ({shooting}), is quick with Pace ({pace}), and strong with Physicality ({physicality}).\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a football analyst AI trained to evaluate players.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "            {\"role\": \"assistant\", \"content\": response}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Helper: Generate ranking example\n",
        "def generate_ranking(n=3):\n",
        "    top_players = df_sorted.head(n)\n",
        "\n",
        "    prompt = f\"List the top {n} players based on Overall rating.\"\n",
        "    response_lines = [\n",
        "        f\"{i+1}. {row['Name']} (Overall: {row['Overall']}, Club: {row['Club']})\"\n",
        "        for i, (_, row) in enumerate(top_players.iterrows())\n",
        "    ]\n",
        "\n",
        "    response = \"\\n\".join(response_lines)\n",
        "\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a football analyst AI trained to rank players.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "            {\"role\": \"assistant\", \"content\": response}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Generate training examples\n",
        "train_examples = [generate_evaluation(row) for _, row in train_df.iterrows()]\n",
        "train_examples.append(generate_ranking(3))\n",
        "train_examples.append(generate_ranking(5))\n",
        "train_examples.append(generate_ranking(10 if len(df) >= 10 else len(df)))\n",
        "\n",
        "# Generate testing examples (no rankings here, just evaluations)\n",
        "test_examples = [generate_evaluation(row) for _, row in test_df.iterrows()]\n",
        "\n",
        "# Save training JSONL\n",
        "with open(\"player_ranking_train.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for ex in train_examples:\n",
        "        json.dump(ex, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "# Save testing JSONL\n",
        "with open(\"player_ranking_test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for ex in test_examples:\n",
        "        json.dump(ex, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(f\"✅ Training examples: {len(train_examples)}\")\n",
        "print(f\"✅ Testing examples: {len(test_examples)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pntNWBrBik4",
        "outputId": "26d447a8-0c1f-427b-f22c-73ce2659683c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training examples: 11094\n",
            "✅ Testing examples: 4754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning Ranking Model"
      ],
      "metadata": {
        "id": "PMcr5zT1xY6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import time\n",
        "\n",
        "# Set API key (can skip this if using env var)\n",
        "openai.api_key = \"your-api-key-here\"  # Or set via env: export OPENAI_API_KEY=\"...\"\n",
        "\n",
        "# Step 1: Upload the JSONL file\n",
        "def upload_file(file_path):\n",
        "    print(\"Uploading dataset...\")\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        file = openai.files.create(file=f, purpose='fine-tune')\n",
        "    print(f\"File uploaded. File ID: {file.id}\")\n",
        "    return file.id\n",
        "\n",
        "# Step 2: Create the fine-tuning job\n",
        "def create_finetune_job(file_id):\n",
        "    print(\"Starting fine-tuning job...\")\n",
        "    job = openai.fine_tuning.jobs.create(\n",
        "        training_file=file_id,\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        suffix=\"player-evaluator\"\n",
        "    )\n",
        "    print(f\"Fine-tune job started. Job ID: {job.id}\")\n",
        "    return job.id\n",
        "\n",
        "# Step 3: Wait for completion (optional)\n",
        "def wait_for_completion(job_id):\n",
        "    print(\"⏳ Waiting for fine-tune job to complete...\")\n",
        "    while True:\n",
        "        job = openai.fine_tuning.jobs.retrieve(job_id)\n",
        "        status = job.status\n",
        "        print(f\"Job status: {status}\")\n",
        "        if status in [\"succeeded\", \"failed\"]:\n",
        "            break\n",
        "        time.sleep(15)\n",
        "\n",
        "    if status == \"succeeded\":\n",
        "        print(f\"Fine-tuning complete! Model ID: {job.fine_tuned_model}\")\n",
        "    else:\n",
        "        print(\"Fine-tuning failed.\")\n",
        "\n",
        "# === RUN ===\n",
        "\n",
        "jsonl_file_path = \"/content/drive/MyDrive/dataset-competition/player_ranking_train.jsonl\"\n",
        "file_id = upload_file(jsonl_file_path)\n",
        "job_id = create_finetune_job(file_id)\n",
        "wait_for_completion(job_id)"
      ],
      "metadata": {
        "id": "yeA65ChGwvCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Accuracy"
      ],
      "metadata": {
        "id": "TQmsk4JoBohC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n",
        "def extract_overall_rating(text):\n",
        "    match = re.search(r\"Overall\\s*\\(?(\\d{2,3})\\)?\", text)\n",
        "    return int(match.group(1)) if match else None\n",
        "\n",
        "def categorize_rating(rating):\n",
        "    if rating is None:\n",
        "        return \"Unknown\"\n",
        "    elif rating >= 90:\n",
        "        return \"High\"\n",
        "    elif rating >= 80:\n",
        "        return \"Mid\"\n",
        "    else:\n",
        "        return \"Low\"\n",
        "\n",
        "def evaluate_finetuned_model_confusion_matrix(test_file_path, model_name, api_key):\n",
        "    llm_model = ChatOpenAI(\n",
        "        model=model_name,\n",
        "        openai_api_key=api_key,\n",
        "        temperature=0  # deterministic output\n",
        "    )\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with open(test_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            sample = json.loads(line)\n",
        "            messages = sample[\"messages\"]\n",
        "            expected_text = messages[-1][\"content\"]\n",
        "            prompt_messages = messages[:-1]\n",
        "\n",
        "            # Convert messages to LangChain format\n",
        "            lc_messages = []\n",
        "            for msg in prompt_messages:\n",
        "                if msg[\"role\"] == \"system\":\n",
        "                    lc_messages.append(SystemMessage(content=msg[\"content\"]))\n",
        "                elif msg[\"role\"] == \"user\":\n",
        "                    lc_messages.append(HumanMessage(content=msg[\"content\"]))\n",
        "\n",
        "            # Extract true label\n",
        "            true_rating = extract_overall_rating(expected_text)\n",
        "            y_true.append(categorize_rating(true_rating))\n",
        "\n",
        "            # Invoke model\n",
        "            try:\n",
        "                response = llm_model.invoke(lc_messages)\n",
        "                predicted_text = response.content\n",
        "                predicted_rating = extract_overall_rating(predicted_text)\n",
        "                y_pred.append(categorize_rating(predicted_rating))\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error: {e}\")\n",
        "                y_pred.append(\"Unknown\")\n",
        "\n",
        "    print(confusion_matrix(y_true, y_pred, labels=[\"High\", \"Mid\", \"Low\"]))"
      ],
      "metadata": {
        "id": "9rxF-FWrj0af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_finetuned_model_confusion_matrix(\n",
        "    test_file_path=\"/content/drive/MyDrive/dataset-competition/player_ranking_test.jsonl\",\n",
        "    model_name=job_info[\"fine_tuned_model\"],\n",
        "    api_key=openai.api_key\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlfM44d1B4BO",
        "outputId": "e05a03ba-1722-40f9-f819-258b679c97c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Metrics::\n",
            "Accuracy : 0.9931\n",
            "Precision: 0.9913\n",
            "Recall   : 0.9945\n",
            "F1 Score : 0.9929\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iAFIYV97aGe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5_kXkVe_aHo6"
      }
    }
  ]
}